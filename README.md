# ğŸ¤– Agentic AI Assistant

An intelligent AI-powered assistant built with LangGraph, LangChain, and Ollama that can handle weather queries, answer general questions using RAG (Retrieval-Augmented Generation), and evaluate its own responses.

## âœ¨ Features

- **ğŸŒ¦ï¸ Weather Integration**: Real-time weather information via OpenWeatherMap API
- **ğŸ“š RAG Pipeline**: Document retrieval and question answering from uploaded PDFs
- **ğŸ§  Intelligent Routing**: Automatically routes queries to appropriate handlers
- **ğŸ¯ Response Evaluation**: Validates and evaluates response quality
- **ğŸ’¬ Chat UI**: Interactive Streamlit web interface with chat history
- **ğŸ“ Document Management**: Upload and ingest PDFs to knowledge base
- **ğŸ” Vector Search**: Semantic search using Qdrant vector database
- **ğŸ“ LLM Processing**: Local LLM inference with Ollama (Ministral 3B model)

## ğŸ—ï¸ Architecture

```
User Query
    â†“
[Decision Node] - Routes query (weather vs general)
    â†“
    â”œâ”€ Weather Route
    â”‚   â”œâ”€ [City Node] - Extract city name
    â”‚   â”œâ”€ [Weather Node] - Fetch weather data
    â”‚   â””â”€ [Answer Node] - Generate response
    â”‚
    â””â”€ General Route
        â”œâ”€ [Context Node] - Retrieve relevant documents (RAG)
        â””â”€ [Answer Node] - Generate response
    â†“
[Evaluation Node] - Validate response quality
    â†“
Final Answer
```

## ğŸ“‹ Prerequisites

- **Python 3.12+**
- **Ollama** (for local LLM inference)
- **Qdrant** (vector database)
- **OpenWeatherMap API Key** (free tier available)


## ğŸš€ Installation

### 1. Clone Repository
```bash
git clone https://github.com/sarveshgulhane/agentic_demo.git
cd agentic_demo
```

### 2. Create Virtual Environment
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Install Development Dependencies (Optional)
```bash
pip install -r requirements-dev.txt
```

### 5. Setup Ollama
```bash
# Install Ollama from https://ollama.ai

# Pull the Ministral 3B model
ollama pull ministral:3b

# Run Ollama server (in a separate terminal)
ollama serve
```

### 6. Setup Qdrant Vector Database
```bash
# Using Docker
docker run -p 6333:6333 -p 6334:6334 \
  -e QDRANT__SERVICE__HTTP_PORT=6333 \
  qdrant/qdrant

```

### 7. Environment Configuration
Create a `.env` file in the project root:

```bash
# OpenWeatherMap API
OPENWEATHER_API_KEY=your_api_key_here

# Qdrant Vector Database
QDRANT_URL=http://localhost:6333

# LangChain Tracing (Optional)
LANGCHAIN_TRACING_V2=false
LANGCHAIN_API_KEY=your_key_here
LANGCHAIN_PROJECT=agentic-demo
```

Get your OpenWeatherMap API key:
1. Visit [OpenWeatherMap](https://openweathermap.org/api)
2. Sign up for free account
3. Generate API key from dashboard
4. Add to `.env`

## ğŸ“ Project Structure

```
agentic_demo/
â”œâ”€â”€ main.py                 # Streamlit web interface
â”œâ”€â”€ config.py              # Configuration management
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ requirements-dev.txt   # Development dependencies
â”‚
â”œâ”€â”€ graph/                 # LangGraph workflow
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ state.py          # Agent state definition
â”‚   â”œâ”€â”€ workflow.py       # Graph workflow setup
â”‚   â””â”€â”€ nodes/            # Individual node implementations
â”‚       â”œâ”€â”€ decision.py   # Route query (weather vs general)
â”‚       â”œâ”€â”€ city.py       # Extract city from query
â”‚       â”œâ”€â”€ weather.py    # Fetch weather data
â”‚       â”œâ”€â”€ context.py    # Retrieve documents (RAG)
â”‚       â”œâ”€â”€ answer.py     # Generate response
â”‚       â””â”€â”€ evaluation.py # Evaluate response quality
â”‚
â”œâ”€â”€ services/             # External service integrations
â”‚   â”œâ”€â”€ llm_service.py    # Ollama LLM interface
â”‚   â”œâ”€â”€ embedding_service.py  # HuggingFace embeddings
â”‚   â””â”€â”€ weather_service.py    # OpenWeatherMap API
â”‚
â”œâ”€â”€ rag/                  # RAG pipeline
â”‚   â”œâ”€â”€ ingestion.py      # PDF ingestion and chunking
â”‚   â””â”€â”€ retriever.py      # Document retrieval
â”‚
â”œâ”€â”€ documents/            # User-uploaded PDFs
â”‚
â”œâ”€â”€ tests/                # Test suite
â”‚   â”œâ”€â”€ conftest.py       # Pytest configuration & fixtures
â”‚   â”œâ”€â”€ test_llm.py       # LLM service tests
â”‚   â”œâ”€â”€ test_rag.py       # RAG/retrieval tests
â”‚   â”œâ”€â”€ test_weather.py   # Weather API tests
â”‚   â”œâ”€â”€ test_decision.py  # Decision routing tests
â”‚   â”œâ”€â”€ test_embedding.py # Embedding tests
â”‚   â”œâ”€â”€ test_integration.py  # End-to-end tests
â”‚   â””â”€â”€ test_examples.py  # Test pattern examples
â”‚
â””â”€â”€ .github/
    â””â”€â”€ workflows/        # CI/CD configuration
```

## ğŸ® Usage

### Start the Web Application
```bash
streamlit run main.py
```

The application will open at `http://localhost:8501`

### Using the Interface

1. **Upload Documents**:
   - Click "ğŸ“„ Document Upload" in sidebar
   - Select PDF files
   - Files are automatically ingested to knowledge base
   - Upload confirmation shows completion

2. **Ask Questions**:
   - Type your query in the chat input
   - Press "ğŸš€ Send" or hit Enter
   - View response in chat history
   - Scroll to see previous conversations

3. **Query Examples**:
   - Weather: "What's the weather in New York?"
   - General: "Explain machine learning"
   - Document-based: "What does the PDF say about AI?"

### API Testing
```bash
# Test individual components
python -c "from services.llm_service import get_llm_response; print(get_llm_response('Hello'))"
```

## ğŸ§ª Testing

### Run All Tests
```bash
pytest
```

### Run with Coverage
```bash
pytest --cov=. --cov-report=html
open htmlcov/index.html
```

## ğŸ”§ Configuration Details

### LLM Configuration
- **Model**: Ministral 3B (3 billion parameters)
- **Temperature**: 0.7 (balanced creativity/consistency)
- **Framework**: LangChain + Ollama

### Embeddings Configuration
- **Model**: all-MiniLM-L6-v2 (HuggingFace)
- **Dimensions**: 384
- **Use Case**: Document semantic search

### RAG Pipeline
- **Chunk Size**: 1000 characters
- **Chunk Overlap**: 200 characters
- **Top-K Retrieval**: 3 documents
- **Vector Database**: Qdrant

## ğŸ”Œ Integration Points

### External APIs
- **OpenWeatherMap**: Weather data
- **HuggingFace Hub**: Embedding models
- **Qdrant API**: Vector operations

### Local Services
- **Ollama**: LLM inference
- **Qdrant**: Vector database

## ğŸ“Š Data Flow

### Weather Query Flow
```
User: "What's the weather in Paris?"
    â†“
Decision Node: Detects weather intent
    â†“
City Node: Extracts "Paris"
    â†“
Weather Service: Calls OpenWeatherMap API
    â†“
Answer Node: Generates response
    â†“
Evaluation Node: Validates quality
    â†“
Response: "It's 15Â°C with clear skies in Paris"
```

### Document Query Flow
```
User: "What is machine learning?"
    â†“
Decision Node: Routes to general/RAG path
    â†“
RAG Retriever: Searches vector database
    â†“
Retrieved Chunks: 3 most relevant documents
    â†“
LLM: Generates answer using documents
    â†“
Evaluation Node: Validates response
    â†“
Response: Answer based on retrieved documents
```


## ğŸ”’ Security Considerations

- Store API keys in `.env` file (not in code)
- Don't commit `.env` to version control
- Use environment variables for sensitive data
- Validate user inputs
- Sanitize file uploads
